# VLLM Distributed Inferencing and Serving on GPU cluser using Kubernetes

This repository provides how to implement vllm distributed inferencing and serving on gpu cluster using kubernetes.

## Install Microk8s

```
sudo snap install microk8s --classic
sudo usermod -aG microk8s $USER
newgrp microk8s
microk8s start
```

## Enable Essenstial Services in Microk8s

```
sudo microk8s enable dashboard cert-manager hostpath-storage ingress metrics-server nvidia
```

## Setup Kubernetes Cluster

```
microk8s add-node
```

Then you can get the "microk8s join" command and you can run it on kubernetes worker node

After that, you need to deploy pod network for communicating between containers.

```
microk8s kubectl apply -f kubernetes/calico.yaml
```

## Deploy VLLM Containers on Kubernetes Cluster

First set the role of kubernetes node

```
microk8s kubectl label nodes ip-10-0-0-29 role=master
microk8s kubectl label nodes ip-10-0-0-6 role=worker-1
```

Then deploy vllm containers on kubernetes cluster

```
microk8s kubectl apply -f kubernetes/namespace.yaml
microk8s kubectl apply -f kubernetes/head-node.yaml
microk8s kubectl apply -f kubernetes/head-service.yaml
microk8s kubectl apply -f kubernetes/worker-node.yaml
microk8s kubectl apply -f kubernetes/vllm-service.yaml
```

This deployment includes setting up Ray cluster for distributed inferencing

## Run VLLM code in container

Check the current running pods in vllm cluster namespace

```
microk8s kubectl get pods -n vllm-cluster
```

Access file system of vllm container if 2 pods are running

```
microk8s kubectl exec -it vllm-head-<xxxxxx> -n vllm-cluster -- /bin/bash
```

Then you can run distributed_batch_inference.py in container

```
python3 distributed_batch_inference.py
```

## Install Prometheus stack
Add prometheus helm repository.
```
microk8s helm3 repo add prometheus-community https://prometheus-community.github.io/helm-charts
microk8s helm3 repo update
```

Modify some settings
```
microk8s helm3 inspect values prometheus-community/kube-prometheus-stack > /tmp/kube-prometheus-stack.values
```

Change the values so that we can expose the service via NodePort so users can access dashboards outside the cluster on a browser.
Look for this section in kube-prometheus-stack.values file.
```
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    ##
    nodePort: 30903
    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
## Additional ports to open for Alertmanager service
    additionalPorts: []
externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    ## Service type
    ##
    type: ClusterIP
```

Change values for nodePort and type .
```
nodePort: 30090
....
type: NodePort
```

Look for serviceMonitorSelectorNilUsesHelmValues and change the value to false.
```
## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    ##
    serviceMonitorSelectorNilUsesHelmValues: false
```

Now search for additionalScrapeSettings â€” this should be empty by default. We will add a configMap to this section. After adding, it should look like this.
```
## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
## as specified in the official Prometheus documentation:
## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
## scrape configs are going to break Prometheus after the upgrade.
##
## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
##
additionalScrapeConfigs:
- job_name: gpu-metrics
  scrape_interval: 1s
  metrics_path: /metrics
  scheme: http
  kubernetes_sd_configs:
  - role: endpoints
    namespaces:
      names:
      - gpu-operator-resources
  relabel_configs:
  - source_labels: [__meta_kubernetes_pod_node_name]
    action: replace
    target_label: kubernetes_node
```

Now save this file. We are now ready to install Prometheus and Grafana. Note that we are creating a new namespace prometheus and installing to it.
```
microk8s helm3 install prometheus-community/kube-prometheus-stack \
   --create-namespace --namespace prometheus \
   --generate-name \
   --values /tmp/kube-prometheus-stack.values
```

You will see that the installation is successful. You can also verify by checking the pods.
```
microk8s kubectl get pods -n prometheus
```

## Install DCGM Exporter

Clone the nvidia gpu monitoring tools from github repository.
```
git clone https://github.com/NVIDIA/gpu-monitoring-tools.git
```

Edit default-counters.csv and dcp-metrics-include.csv files under gpu-monitoring-tools/etc/dcgm-exporter folder, to add metrics we want.
Look for DCGM_FI_DEV_GPU_UTIL tag under Utilization section in the csv file. Uncomment this line.

Now under the folder gpu-monitoring-tools/deployment , execute the following command to install dcgm-exporter.
```
cd gpu-monitoring-tools/deployment
microk8s helm3 install --generate-name ./dcgm-exporter/
```

## Configure Grafana
Change the default ClusterIP exposure that Grafana uses to NodePort , so that we can access Grafana dashboards as well on a browser outside the cluster.
Make a patch file.
```
cat << EOF | tee grafana-patch.yaml
spec:
  type: NodePort
  nodePort: 32322
EOF
```
Now patch the service up with this yaml. You will see a message saying the patch was successful.
```
microk8s kubectl patch svc kube-prometheus-stack-1603211794-grafana \
   -n prometheus \
   --patch "$(cat grafana-patch.yaml)"
```
Note down the external accessible port.
```
microk8s kubectl get svc -n prometheus|grep grafana
```
## Grafana Dashboard
Open http://<machine-ip-address>:<port> on your browser.
The username is ```admin``` and password is ```prom-operator```.

In the Grafana page, go to Dashboards -> Manage -> Import. In the resulting page, add https://grafana.com/grafana/dashboards/12239 as url.

Click Load and select Prometheus as data source. Once imported, you can watch the new dashboard.
